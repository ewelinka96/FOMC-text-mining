---
title: "Text mining project"
author: "Krystian Andruszek, Ewelina Osowska and Ewa Sobolewska"
date: "13 01 2020"
output:
  rmarkdown::html_document: 
    theme: yeti
    toc: true
    toc_float: true
    toc_depth: 2
    code_folding: hide
---


```{r warning=FALSE, message=FALSE}
# Loading packages
library(readtext)
library(readxl)
library(dplyr)
library(tidyverse)
library(tidytext)
library(tm)
library(textstem)
library(wordcloud)
library(slam)
library(topicmodels)
library(SentimentAnalysis)
```

# Introduction
(...)

# Background
(...)

## What is Federal Open Market Committee?
Federal Open Market Committee (FOMC) is the body of the central bank of United States (the Federal Reserve System). Its main duties is setting the national monetary policy. The FOMC makes all decisions regarding the federal funds rate, the size and composition of the Federal Reserve’s asset holdings, and communications with the public about the likely future course of monetary policy. The FOMC consists of 12 voting members: seven members of the Board of Governors, the president of the Federal Reserve Bank of New York and 4 of the remaining 11 Reserve Bank presidents, who serve one-year terms on a rotating basis. All 12 of the Reserve Bank presidents attend FOMC meetings and participate in FOMC discussions, but only the presidents who are Committee members at the time may vote on policy decisions. FOMC meetings typically are held eight times each year in Washington, D.C., and at other times as needed. 

## How are statements organized?
Immediately after each FOMC meeting, (...)


# Statistical analysis

(...)

## Data description

(...)
https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm
+ algorytm skrapowania


## Text preparation

(...)

```{r}
# Loading scrapped statements (from 2006 to 2018)
DATA_DIR <- "C:/Users/KAndr/OneDrive/Studia/II rok I semestr/Text mining/Text mining project/Statements/"
fomc_2006 <- readtext(paste0(DATA_DIR, "/2006/*"))
fomc_2007 <- readtext(paste0(DATA_DIR, "/2007/*"))
fomc_2008 <- readtext(paste0(DATA_DIR, "/2008/*"))
fomc_2009 <- readtext(paste0(DATA_DIR, "/2009/*"))
fomc_2010 <- readtext(paste0(DATA_DIR, "/2010/*"))
fomc_2011 <- readtext(paste0(DATA_DIR, "/2011/*"))
fomc_2012 <- readtext(paste0(DATA_DIR, "/2012/*"))
fomc_2013 <- readtext(paste0(DATA_DIR, "/2013/*"))
fomc_2014 <- readtext(paste0(DATA_DIR, "/2014/*"))
fomc_2015 <- readtext(paste0(DATA_DIR, "/2015/*"))
fomc_2016 <- readtext(paste0(DATA_DIR, "/2016/*"))
fomc_2017 <- readtext(paste0(DATA_DIR, "/2017/*"))
fomc_2018 <- readtext(paste0(DATA_DIR, "/2018/*"))
# Binding data
statements <- rbind(fomc_2006, fomc_2007, fomc_2008, fomc_2009, fomc_2010, fomc_2011,
                    fomc_2012, fomc_2013, fomc_2014, fomc_2015, fomc_2016, fomc_2017, fomc_2018)
# Removing files from memory
remove(fomc_2006, fomc_2007, fomc_2008, fomc_2009, fomc_2010, fomc_2011,
       fomc_2012, fomc_2013, fomc_2014, fomc_2015, fomc_2016, fomc_2017, fomc_2018)
```

```{r}
# Initial preprocessing
statements <- statements %>% mutate(ID = 1:n())
colnames(statements) <- c("Date", "Text", "ID")
statements$Date <- gsub(".txt", "", statements$Date)
statements$Date <- as.Date(statements$Date, "%Y%m%d ")
statements_all <- as.vector(statements$Text)
length(statements_all) # 107 documents
```

```{r}
# Converting documents into corpus
(corpus_all <- VCorpus(VectorSource(statements_all)))
```

```{r}
inspect(corpus_all[[1]])
```

```{r}
as.character(corpus_all[[1]]) 
```

## Preprocessing

```{r}
stopwords <- stopwords("en")
```

```{r}
system.time (
  corpus_clean <- corpus_all %>% 
    tm_map(tolower) %>%
    tm_map(removeWords, stopwords) %>% 
    tm_map(removePunctuation) %>%
    tm_map(removeNumbers)  %>%
    tm_map(stripWhitespace) %>% 
    tm_map(PlainTextDocument)
)
```

```{r}
# example statement after cleaning
as.character(corpus_clean[[1]]) 
```

```{r}
# example statement after cleaning
as.character(corpus_clean[[2]]) 
```

```{r}
df_corpus <- data.frame(text = unlist(sapply(corpus_clean, `[`, "content")), stringsAsFactors = F)
df_corpus <- df_corpus %>% mutate(doc_id = 1:n())
```


```{r}
statements_clean <- statements %>% 
  mutate(cleaned_text = df_corpus$text)

count_cleaned_word <- statements_clean %>%
  unnest_tokens(word_count, cleaned_text) %>%
  count(ID, word_count, sort = T) %>% 
  group_by(ID) %>% 
  summarize(word_cleaned_count = sum(n))

statements_clean_count <- left_join(statements_clean, count_cleaned_word, by = 'ID')

count_word <- statements_clean_count %>%
  unnest_tokens(word_count, Text) %>%
  count(ID, word_count, sort = T) %>% 
  group_by(ID) %>% 
  summarize (word_count = sum(n))

statements_final <- left_join(statements_clean_count, count_word, by = 'ID')
```

## Word counts over time

```{r}
library(plotly)
myplot <- statements_final %>% 
              select(word_count, Date) %>% 
              ggplot() +
              geom_line(aes(x=Date, y= word_count)) + 
              theme_minimal()
ggplotly(myplot)
```


## Wordclouds

```{r}
dtm <- TermDocumentMatrix(corpus_clean)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)

set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=50, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```


## Associations and network analysis

















## Sentiment analysis

```{r}
# Lemmatization
statements_final$lemma_text <- lemmatize_strings(statements_final$cleaned_text)
```

```{r}
# Tokenization
tokens <- statements_final %>%
  unnest_tokens(word, lemma_text) 
```

## Topic modelling

```{r}
# topic modelling - do poprawy na pewno, bo słabo wychodzi

# install.packages("topicmodels")

# Creating a Term document Matrix
tdm = DocumentTermMatrix(corpus_clean) 

# create tf-idf matrix
term_tfidf <- tapply(tdm$v/row_sums(tdm)[tdm$i], tdm$j, mean) * log2(nDocs(tdm)/col_sums(tdm > 0))
summary(term_tfidf)
tdm <- tdm[,term_tfidf >= 0.05]
tdm <- tdm[row_sums(tdm) > 0,]
summary(col_sums(tdm))
# finding best K 
best.model <- lapply(seq(2, 50, by = 1), function(d){LDA(tdm, d)})
best.model.logLik <- as.data.frame(as.matrix(lapply(best.model, logLik)))
# calculating LDA
k = 5 # number of topics
SEED = 112 # number of documents 
CSC_TM <-list(VEM = LDA(tdm, k = k, 
                        control = list(seed = SEED)), 
              VEM_fixed = LDA(tdm, k = k,
                              control = list(estimate.alpha = FALSE, seed = SEED)),
              Gibbs = LDA(tdm, k = k, method = "Gibbs",
                          control = list(seed = SEED, burnin = 1000, thin = 100, iter = 1000)),
              CTM = CTM(tdm, k = k,
                        control = list(seed = SEED,
                                       var = list(tol = 10^-4), 
                                       em = list(tol = 10^-3))))

sapply(CSC_TM[1:2], slot, "alpha")
sapply(CSC_TM, function(x) mean(apply(posterior(x)$topics, 1, function(z) sum(z*log(z)))))
Topic <- topics(CSC_TM[["VEM"]], 1)
Terms <- terms(CSC_TM[["VEM"]], 8)
Terms
```








