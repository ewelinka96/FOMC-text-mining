---
title: "Text mining project"
author: "Krystian Andruszek, Ewelina Osowska and Ewa Sobolewska"
date: "13 01 2020"
output:
  rmarkdown::html_document: 
    theme: yeti
    toc: true
    toc_float: true
    toc_depth: 2
    code_folding: hide
---


```{r warning=FALSE, message=FALSE}
# Loading packages
library(readtext)
library(readxl)
library(dplyr)
library(tidyverse)
library(tidytext)
library(tm)
library(textstem)
library(wordcloud)
library(slam)
library(topicmodels)
library(SentimentAnalysis)
```

## Introduction

The main objective of this paper is to analyse the Federal Open Market Commitee statements using text mining methods and tools provided by R. We start with basic analysis of (....)

## What is Federal Open Market Committee?

Federal Open Market Committee (FOMC) is the body of the central bank of United States (the Federal Reserve System). Its main duties is setting the national monetary policy. The FOMC holds eight regularly scheduled meetings per year. At these meetings, the Committee reviews economic and financial conditions, determines the appropriate stance of monetary policy, and assesses the risks to its long-run goals of price stability and sustainable economic growth. The FOMC consists of 12 voting members: seven members of the Board of Governors, the president of the Federal Reserve Bank of New York and 4 of the remaining 11 Reserve Bank presidents, who serve one-year terms on a rotating basis. All 12 of the Reserve Bank presidents attend FOMC meetings and participate in FOMC discussions, but only the presidents who are Committee members at the time may vote on policy decisions. FOMC meetings typically are held eight times each year in Washington, D.C., and at other times as needed. 

## How are statements organized?

The Committee releases a public statement immediately after each FOMC meeting. Each statement follows very similar structure. Firstly, the general background of the economic situation is presented. Then the Commitee introduces the value of the established federal funds rate and also share predictions. At the end, there are listed names of people which voted for the FOMC monetary policy action.

## Data description

We sourced the data by scraping the statements from the Federal Reserve official website [^fomc] using Python. In the scraping algorithm we limited the content only to FOMC announcment, omitting the names of voters listed in the last paragraph. The analysed period includes years from 2006 to 2018 which resulted in obtaining 107 documents. 

```{r}
# Loading scrapped statements
# DATA_DIR <- "C:/Users/KAndr/OneDrive/Studia/II rok I semestr/Text mining/Text mining project/Statements/"
# DATA_DIR <- "C:/Users/KAndr/OneDrive/Studia/II rok I semestr/Text mining/Text mining project/Statements/"
DATA_DIR <- "~/Desktop/FOMC-text-mining/Statements"

fomc_2006 <- readtext(paste0(DATA_DIR, "/2006/*"))
fomc_2007 <- readtext(paste0(DATA_DIR, "/2007/*"))
fomc_2008 <- readtext(paste0(DATA_DIR, "/2008/*"))
fomc_2009 <- readtext(paste0(DATA_DIR, "/2009/*"))
fomc_2010 <- readtext(paste0(DATA_DIR, "/2010/*"))
fomc_2011 <- readtext(paste0(DATA_DIR, "/2011/*"))
fomc_2012 <- readtext(paste0(DATA_DIR, "/2012/*"))
fomc_2013 <- readtext(paste0(DATA_DIR, "/2013/*"))
fomc_2014 <- readtext(paste0(DATA_DIR, "/2014/*"))
fomc_2015 <- readtext(paste0(DATA_DIR, "/2015/*"))
fomc_2016 <- readtext(paste0(DATA_DIR, "/2016/*"))
fomc_2017 <- readtext(paste0(DATA_DIR, "/2017/*"))
fomc_2018 <- readtext(paste0(DATA_DIR, "/2018/*"))
# Binding data
statements <- rbind(fomc_2006, fomc_2007, fomc_2008, fomc_2009, fomc_2010, fomc_2011,
                    fomc_2012, fomc_2013, fomc_2014, fomc_2015, fomc_2016, fomc_2017, fomc_2018)
# Removing files from memory
remove(fomc_2006, fomc_2007, fomc_2008, fomc_2009, fomc_2010, fomc_2011,
       fomc_2012, fomc_2013, fomc_2014, fomc_2015, fomc_2016, fomc_2017, fomc_2018)
```

## Text preparation

We start our work on statments with the initial preprocessing of the dataset. It consists of two columns: doc_id and text. Doc_id is sourced from each statement's website link. Text is just a content of the statement. 

```{r}
head(statements, 1)
```

```{r}
# adding an unique ID
statements <- statements %>% mutate(ID = 1:n())
# setting column names 
colnames(statements) <- c("Date", "Text", "ID")
# modification of doc_id column - changing it to date column
statements$Date <- gsub(".txt", "", statements$Date)
statements$Date <- as.Date(statements$Date, "%Y%m%d ")
```

```{r}
statements_all <- as.vector(statements$Text)
length(statements_all) 
```

The next step was concerting the dataset into volatile corpora which is a handful form in the following operations. Below can be seen an example statement before any text preprocessing operations applied.

```{r}
(corpus_all <- VCorpus(VectorSource(statements_all)))
```

```{r}
inspect(corpus_all[[1]])
```

## Preprocessing

We start preprocessing with text cleaning using tm_map() function. We lower each case, remove words from the built-in stopwords list, we remove punctuation, unnecessary whitespaces and numbers. At the end we apply PlainTextDocument() function.

```{r}
corpus_clean <- corpus_all %>% 
    tm_map(tolower) %>%
    tm_map(removeWords, stopwords("en")) %>% 
    tm_map(removePunctuation) %>%
    tm_map(stripWhitespace) %>% 
    tm_map(removeNumbers) %>% 
    tm_map(PlainTextDocument)
```

Below can be seen examples of the statements after above cleaning steps.

```{r}
as.character(corpus_clean[[1]]) 
```

```{r}
# example statement after cleaning
as.character(corpus_clean[[2]]) 
```

In order to ease operations on the corpus, we modify it into a data frame. 

```{r}
df_corpus <- data.frame(text = unlist(sapply(corpus_clean, `[`, "content")), stringsAsFactors = F)
df_corpus <- df_corpus %>% mutate(doc_id = 1:n())
```

```{r}
df_corpus$text[1]
```

In the next steps, we append statements data frame with cleaned text. We also count number of words occuring in the original statement and in the cleaned statement. 

```{r}
statements_clean <- statements %>% 
  mutate(cleaned_text = df_corpus$text)
```

```{r}
# clened_text
count_cleaned_word <- statements_clean %>%
  unnest_tokens(word_count, cleaned_text) %>%
  count(ID, word_count, sort = T) %>% 
  group_by(ID) %>%
  summarize(word_cleaned_count = sum(n))

statements_clean_count <- left_join(statements_clean, count_cleaned_word, by = 'ID')
```

```{r}
count_word <- statements_clean_count %>%
  unnest_tokens(word_count, Text) %>%
  count(ID, word_count, sort = T) %>% 
  group_by(ID) %>% 
  summarize (word_count = sum(n))

statements_final <- left_join(statements_clean_count, count_word, by = 'ID')
```



## Word counts over time

(...)

```{r}
library(plotly)
library(dplyr)
library(viridis)
myplot <- statements_final %>% 
              select(Date, word_count, word_cleaned_count) %>% 
              ggplot() +
              geom_line(aes(x = Date, 
                            y = word_count), 
                        color = viridis(10)[3]) + 
              geom_line(aes(x = Date, 
                            y = word_cleaned_count), 
                        color = viridis(10)[6]) +
              labs(x = "Date", 
                   y = "Number of words", 
                   title = "Comparison of number of words between original and cleaned <br>statements content over time") +
              scale_x_date(date_breaks = "1 year", 
                           date_labels = "%Y") +
              theme_minimal()

ggplotly(myplot)
```


## TF-IDF

(...)

```{r}
library(lubridate)
statements_words <- statements_clean_count %>%
  mutate(year = year(Date)) %>% 
  unnest_tokens(word_count, cleaned_text) %>%
  count(year, word_count, sort = T)

statements_words
```

```{r}
statements_words <- statements_words %>%
  bind_tf_idf(word_count, year, n)

statements_words
```

```{r, fig.height=15}
pd = statements_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word_count, levels = rev(unique(word_count)))) %>% 
  group_by(year) %>% 
  top_n(10) %>% 
  ungroup() %>%
  arrange(year, tf_idf) %>%
  mutate(order = row_number()) 

ggplot(pd, aes(order, tf_idf, fill = year)) +
  geom_bar(show.legend = FALSE, stat = "identity") +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~year, ncol = 3, scales = "free") +
  scale_x_continuous(breaks = pd$order, 
                     labels = pd$word,
                     expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  coord_flip() +
  theme_minimal()
```



## Wordclouds

```{r}
library(wordcloud)
dtm <- TermDocumentMatrix(corpus_clean)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing=TRUE)
d <- data.frame(word = names(v), freq=v)
head(d, 10)
```

```{r}
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=50, random.order=FALSE, rot.per=0.35, 
          colors=viridis(10))
```



## Associations and network analysis

















## Sentiment analysis

```{r}
# Lemmatization
statements_final$lemma_text <- lemmatize_strings(statements_final$cleaned_text)
```

```{r}
# Tokenization
tokens <- statements_final %>%
  unnest_tokens(word, lemma_text) 
```

## Topic modelling

```{r}
# topic modelling - do poprawy na pewno, bo s≈Çabo wychodzi

# install.packages("topicmodels")

# Creating a Term document Matrix
tdm = DocumentTermMatrix(corpus_clean) 

# create tf-idf matrix
term_tfidf <- tapply(tdm$v/row_sums(tdm)[tdm$i], tdm$j, mean) * log2(nDocs(tdm)/col_sums(tdm > 0))
summary(term_tfidf)
tdm <- tdm[,term_tfidf >= 0.05]
tdm <- tdm[row_sums(tdm) > 0,]
summary(col_sums(tdm))
# finding best K 
best.model <- lapply(seq(2, 50, by = 1), function(d){LDA(tdm, d)})
best.model.logLik <- as.data.frame(as.matrix(lapply(best.model, logLik)))
# calculating LDA
k = 5 # number of topics
SEED = 112 # number of documents 
CSC_TM <-list(VEM = LDA(tdm, k = k, 
                        control = list(seed = SEED)), 
              VEM_fixed = LDA(tdm, k = k,
                              control = list(estimate.alpha = FALSE, seed = SEED)),
              Gibbs = LDA(tdm, k = k, method = "Gibbs",
                          control = list(seed = SEED, burnin = 1000, thin = 100, iter = 1000)),
              CTM = CTM(tdm, k = k,
                        control = list(seed = SEED,
                                       var = list(tol = 10^-4), 
                                       em = list(tol = 10^-3))))

sapply(CSC_TM[1:2], slot, "alpha")
sapply(CSC_TM, function(x) mean(apply(posterior(x)$topics, 1, function(z) sum(z*log(z)))))
Topic <- topics(CSC_TM[["VEM"]], 1)
Terms <- terms(CSC_TM[["VEM"]], 8)
Terms
```




[^fomc]: Source: https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm.



