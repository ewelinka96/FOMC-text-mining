fomc_2012, fomc_2013, fomc_2014, fomc_2015, fomc_2016, fomc_2017, fomc_2018)
head(statements, 1)
# adding an unique ID
statements <- statements %>% mutate(ID = 1:n())
# setting column names
colnames(statements) <- c("Date", "Text", "ID")
# modification of doc_id column - changing it to date column
statements$Date <- gsub(".txt", "", statements$Date)
statements$Date <- as.Date(statements$Date, "%Y%m%d ")
statements_all <- as.vector(statements$Text)
length(statements_all)
(corpus_all <- VCorpus(VectorSource(statements_all)))
inspect(corpus_all[[1]])
corpus_clean <- corpus_all %>%
tm_map(tolower) %>%
tm_map(removeWords, stopwords("en")) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace) %>%
tm_map(removeNumbers) %>%
tm_map(PlainTextDocument)
as.character(corpus_clean[[1]])
df_corpus <- data.frame(text = unlist(sapply(corpus_clean, `[`, "content")), stringsAsFactors = F)
df_corpus <- df_corpus %>% mutate(doc_id = 1:n())
df_corpus$text[1]
statements_clean <- statements %>%
mutate(cleaned_text = df_corpus$text)
# Loading packages
library(readtext)
library(readxl)
library(dplyr)
library(tidyverse)
library(tidytext)
library(tm)
library(textstem)
library(wordcloud)
library(slam)
library(topicmodels)
library(SentimentAnalysis)
# Loading scrapped statements
# DATA_DIR <- "C:/Users/KAndr/OneDrive/Studia/II rok I semestr/Text mining/Text mining project/Statements/"
# DATA_DIR <- "~/FOMC-text-mining/Statements"
DATA_DIR <- "~/Desktop/FOMC-text-mining/Statements"
fomc_2006 <- readtext(paste0(DATA_DIR, "/2006/*"))
fomc_2007 <- readtext(paste0(DATA_DIR, "/2007/*"))
fomc_2008 <- readtext(paste0(DATA_DIR, "/2008/*"))
fomc_2009 <- readtext(paste0(DATA_DIR, "/2009/*"))
fomc_2010 <- readtext(paste0(DATA_DIR, "/2010/*"))
fomc_2011 <- readtext(paste0(DATA_DIR, "/2011/*"))
fomc_2012 <- readtext(paste0(DATA_DIR, "/2012/*"))
fomc_2013 <- readtext(paste0(DATA_DIR, "/2013/*"))
fomc_2014 <- readtext(paste0(DATA_DIR, "/2014/*"))
fomc_2015 <- readtext(paste0(DATA_DIR, "/2015/*"))
fomc_2016 <- readtext(paste0(DATA_DIR, "/2016/*"))
fomc_2017 <- readtext(paste0(DATA_DIR, "/2017/*"))
fomc_2018 <- readtext(paste0(DATA_DIR, "/2018/*"))
# Binding data
statements <- rbind(fomc_2006, fomc_2007, fomc_2008, fomc_2009, fomc_2010, fomc_2011,
fomc_2012, fomc_2013, fomc_2014, fomc_2015, fomc_2016, fomc_2017, fomc_2018)
# Removing files from memory
remove(fomc_2006, fomc_2007, fomc_2008, fomc_2009, fomc_2010, fomc_2011,
fomc_2012, fomc_2013, fomc_2014, fomc_2015, fomc_2016, fomc_2017, fomc_2018)
head(statements, 1)
# adding an unique ID
statements <- statements %>% mutate(ID = 1:n())
# setting column names
colnames(statements) <- c("Date", "Text", "ID")
# modification of doc_id column - changing it to date column
statements$Date <- gsub(".txt", "", statements$Date)
statements$Date <- as.Date(statements$Date, "%Y%m%d ")
statements_all <- as.vector(statements$Text)
length(statements_all)
(corpus_all <- VCorpus(VectorSource(statements_all)))
inspect(corpus_all[[1]])
corpus_clean <- corpus_all %>%
tm_map(tolower) %>%
tm_map(removeWords, stopwords("en")) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace) %>%
tm_map(removeNumbers) %>%
tm_map(PlainTextDocument)
as.character(corpus_clean[[1]])
df_corpus <- data.frame(text = unlist(sapply(corpus_clean, `[`, "content")), stringsAsFactors = F)
df_corpus <- df_corpus %>% mutate(doc_id = 1:n())
df_corpus$text[1]
statements_clean <- statements %>%
mutate(cleaned_text = df_corpus$text)
statements_clean$cleaned_text <- lemmatize_strings(statements_clean$cleaned_text)
statements_clean$cleaned_text[1]
library(plotly)
library(dplyr)
library(viridis)
myplot <- statements_final %>%
select(Date, word_count, word_cleaned_count) %>%
ggplot() +
geom_line(aes(x = Date,
y = word_count),
color = viridis(10)[3]) +
geom_line(aes(x = Date,
y = word_cleaned_count),
color = viridis(10)[6]) +
labs(x = "Date",
y = "Number of words",
title = "Comparison of number of words between original and cleaned <br>statements content over time") +
scale_x_date(date_breaks = "1 year",
date_labels = "%Y") +
theme_minimal()
count_cleaned_word <- statements_clean %>%
unnest_tokens(word_count, cleaned_text) %>%
count(ID, word_count, sort = T) %>%
group_by(ID) %>%
summarize(word_cleaned_count = sum(n))
statements_clean_count <- left_join(statements_clean, count_cleaned_word, by = 'ID')
count_word <- statements_clean_count %>%
unnest_tokens(word_count, Text) %>%
count(ID, word_count, sort = T) %>%
group_by(ID) %>%
summarize (word_count = sum(n))
statements_final <- left_join(statements_clean_count, count_word, by = 'ID')
library(plotly)
library(dplyr)
library(viridis)
myplot <- statements_final %>%
select(Date, word_count, word_cleaned_count) %>%
ggplot() +
geom_line(aes(x = Date,
y = word_count),
color = viridis(10)[3]) +
geom_line(aes(x = Date,
y = word_cleaned_count),
color = viridis(10)[6]) +
labs(x = "Date",
y = "Number of words",
title = "Comparison of number of words between original and cleaned <br>statements content over time") +
scale_x_date(date_breaks = "1 year",
date_labels = "%Y") +
theme_minimal()
ggplotly(myplot)
library(ggplot2)
library(dplyr)
library(lubridate)
word_counts_zipf <- statements_clean_count %>%
mutate(year = year(Date)) %>%
unnest_tokens(word_count, cleaned_text) %>%
count( word_count, sort = T)
word_count <- word_counts_zipf# Data frame containing words and their frequency
colnames(word_count) <- c("word", "count")
alpha <- 1 # Change it needed
word_count <- word_count %>%
mutate(word = factor(word, levels = word),
rank = row_number(),
zipfs_freq = ifelse(rank == 1, count, dplyr::first(count) / rank^alpha))
word_count
p1 <- ggplot(word_count,
aes(x = rank, y = count,
color = rank,
text = paste("Word: ", word, "<br>Count: ", count))) +
geom_point() +
labs(x = "rank", y = "count", title = "Zipf's law visualization") +
scale_color_viridis_c() +
geom_vline(xintercept = 10) +
geom_vline(xintercept = 300) +
theme_minimal() +
theme(legend.position = "none")
ggplotly(p1, tooltip = "text")
p1 <- ggplot(word_count,
aes(x = rank, y = count,
color = rank,
text = paste("Word: ", word, "<br>Frequency of word: ", count))) +
geom_point() +
labs(x = "Rank", y = "count", title = "Zipf's law visualization") +
scale_color_viridis_c() +
geom_vline(xintercept = 10) +
geom_vline(xintercept = 300) +
theme_minimal() +
theme(legend.position = "none")
ggplotly(p1, tooltip = "text")
p1 <- ggplot(word_count,
aes(x = rank, y = count,
color = rank,
text = paste("Word: ", word,
"<br>Frequency of word: ", count,
"<br>Rank: ", rank))) +
geom_point() +
labs(x = "Rank", y = "count", title = "Zipf's law visualization") +
scale_color_viridis_c() +
geom_vline(xintercept = 10) +
geom_vline(xintercept = 300) +
theme_minimal() +
theme(legend.position = "none")
ggplotly(p1, tooltip = "text")
p1 <- ggplot(word_count,
aes(x = rank, y = count,
color = rank,
text = paste("Word: ", word,
"<br>Frequency of word: ", count,
"<br>Rank: ", rank))) +
geom_point() +
labs(x = "Rank", y = "count", title = "Zipf's law visualization") +
scale_color_viridis_c() +
geom_vline(xintercept = 17) +
geom_vline(xintercept = 300) +
theme_minimal() +
theme(legend.position = "none")
ggplotly(p1, tooltip = "text")
large_zipf <- as.vector(word_count$word[1:17])
small_zipf <- as.vector(word_count$word[300:1174])
corpus_clean <- corpus_clean %>% tm_map(removeWords, large_zipf)
corpus_clean <- corpus_clean %>% tm_map(removeWords, small_zipf)
df_corpus <- data.frame(text = unlist(sapply(corpus_clean, `[`, "content")), stringsAsFactors = F)
df_corpus <- df_corpus %>% mutate(doc_id = 1:n())
statements_clean <- statements %>%
mutate(cleaned_text = df_corpus$text)
large_zipf <- as.vector(word_count$word[1:17])
small_zipf <- as.vector(word_count$word[300:1174])
corpus_clean <- corpus_clean %>% tm_map(removeWords, large_zipf)
corpus_clean <- corpus_clean %>% tm_map(removeWords, small_zipf)
df_corpus <- data.frame(text = unlist(sapply(corpus_clean, `[`, "content")), stringsAsFactors = F)
df_corpus <- df_corpus %>% mutate(doc_id = 1:n())
statements_clean <- statements %>%
mutate(cleaned_text = df_corpus$text)
statements_clean$cleaned_text <- lemmatize_strings(statements_clean$cleaned_text)
pd <- statements_words %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word_count, levels = rev(unique(word_count)))) %>%
group_by(year) %>%
top_n(10) %>%
ungroup() %>%
arrange(year, tf_idf) %>%
mutate(order = row_number())
statements_words <- statements_clean %>%
mutate(year = year(Date)) %>%
unnest_tokens(word_count, cleaned_text) %>%
count(year, word_count, sort = T)
statements_words <- statements_words %>%
bind_tf_idf(word_count, year, n) %>%
arrange(desc(tf_idf))
statements_words
pd <- statements_words %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word_count, levels = rev(unique(word_count)))) %>%
group_by(year) %>%
top_n(10) %>%
ungroup() %>%
arrange(year, tf_idf) %>%
mutate(order = row_number())
ggplot(pd, aes(order, tf_idf, fill = tf_idf)) +
geom_bar(show.legend = FALSE, stat = "identity") +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~year, ncol = 2, scales = "free") +
scale_x_continuous(breaks = pd$order,
labels = pd$word,
expand = c(0,0)) +
scale_y_continuous(expand = c(0,0)) +
coord_flip() +
theme_minimal() +
scale_fill_viridis_c(direction=-1)
pd <- statements_words %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word_count, levels = rev(unique(word_count)))) %>%
group_by(year) %>%
top_n(10) %>%
ungroup() %>%
arrange(year, tf_idf) %>%
mutate(order = row_number())
ggplot(pd, aes(order, tf_idf, fill = tf_idf)) +
geom_bar(show.legend = FALSE, stat = "identity") +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~year, ncol = 2, scales = "free") +
scale_x_continuous(breaks = pd$order, labels = pd$word, expand = c(0,0)) +
scale_y_continuous(expand = c(0,0)) +
coord_flip() +
theme_minimal() +
scale_fill_viridis_c(direction=-1)
pd <- statements_words %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word_count, levels = rev(unique(word_count)))) %>%
group_by(year) %>%
top_n(10) %>%
ungroup() %>%
arrange(year, tf_idf) %>%
mutate(order = row_number())
ggplot(pd, aes(order, tf_idf, fill = tf_idf)) +
geom_bar(show.legend = FALSE, stat = "identity") +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~year, ncol = 2, scales = "free") +
scale_x_continuous(breaks = pd$order, labels = pd$word, expand = c(0,0)) +
scale_y_continuous(expand = c(0,0)) +
coord_flip() +
theme_minimal() +
scale_fill_viridis_c(direction=-1)
statements_words <- statements_clean %>%
mutate(year = year(Date)) %>%
unnest_tokens(word_count, cleaned_text) %>%
count(ID, year, word_count, sort = T)
statements_words <- statements_words %>%
bind_tf_idf(word_count, ID, year, n) %>%
arrange(desc(tf_idf))
statements_words <- statements_clean %>%
mutate(year = year(Date)) %>%
unnest_tokens(word_count, cleaned_text) %>%
count(ID, year, word_count, sort = T)
statements_words_id <- statements_words %>%
bind_tf_idf(word_count, ID, n) %>%
arrange(desc(tf_idf))
statements_words
statements_words <- statements_clean %>%
mutate(year = year(Date)) %>%
unnest_tokens(word_count, cleaned_text) %>%
count(ID, year, word_count, sort = T)
statements_words_id <- statements_words %>%
bind_tf_idf(word_count, ID, n) %>%
arrange(desc(tf_idf))
statements_words_id
pd <- statements_words %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word_count, levels = rev(unique(word_count)))) %>%
group_by(year) %>%
top_n(10) %>%
ungroup() %>%
arrange(year, tf_idf) %>%
mutate(order = row_number())
statements_words_year <- statements_words %>%
bind_tf_idf(word_count, year, n) %>%
arrange(desc(tf_idf))
pd <- statements_words_year %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word_count, levels = rev(unique(word_count)))) %>%
group_by(year) %>%
top_n(10) %>%
ungroup() %>%
arrange(year, tf_idf) %>%
mutate(order = row_number())
ggplot(pd, aes(order, tf_idf, fill = tf_idf)) +
geom_bar(show.legend = FALSE, stat = "identity") +
labs(x = NULL, y = "TF-IDF value") +
facet_wrap(~year, ncol = 2, scales = "free") +
scale_x_continuous(breaks = pd$order, labels = pd$word, expand = c(0,0)) +
scale_y_continuous(expand = c(0,0)) +
coord_flip() +
theme_minimal() +
scale_fill_viridis_c(direction=-1)
View(statements_words)
statements_words_year <- statements_words %>%
group_by(year, word_count) +
bind_tf_idf(word_count, year, n) %>%
arrange(desc(tf_idf))
statements_words %>%
group_by(year, word_count)
statements_words_year <- statements_words %>%
group_by(year, word_count) %>% sum %>%
bind_tf_idf(word_count, year, n) %>%
arrange(desc(tf_idf))
statements_words_year <- statements_words %>%
group_by(year, word_count) %>%
sum() %>%
bind_tf_idf(word_count, year, n) %>%
arrange(desc(tf_idf))
statements_words_year <- statements_words %>%
group_by(year, word_count) %>%
summarize(n = sum(n)) %>%
bind_tf_idf(word_count, year, n) %>%
arrange(desc(tf_idf))
statements_words_year <- statements_words %>%
group_by(year, word_count) %>%
summarize(n = sum(n)) %>%
bind_tf_idf(word_count, year, n) %>%
arrange(desc(tf_idf)) %>%
ungroup()
View(statements_words_year)
statements_words_year <- statements_words %>%
group_by(year, word_count) %>%
summarize(n = sum(n)) %>%
bind_tf_idf(word_count, year, n) %>%
arrange(desc(tf_idf)) %>%
ungroup()
pd <- statements_words_year %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word_count, levels = rev(unique(word_count)))) %>%
group_by(year) %>%
top_n(10) %>%
ungroup() %>%
arrange(year, tf_idf) %>%
mutate(order = row_number())
ggplot(pd, aes(order, tf_idf, fill = tf_idf)) +
geom_bar(show.legend = FALSE, stat = "identity") +
labs(x = NULL, y = "TF-IDF value") +
facet_wrap(~year, ncol = 2, scales = "free") +
scale_x_continuous(breaks = pd$order, labels = pd$word, expand = c(0,0)) +
scale_y_continuous(expand = c(0,0)) +
coord_flip() +
theme_minimal() +
scale_fill_viridis_c(direction=-1)
statements_words_year <- statements_words %>%
group_by(year, word_count) %>%
summarize(n = sum(n)) %>%
bind_tf_idf(word_count, year, n) %>%
arrange(desc(tf_idf))
pd <- statements_words_year %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word_count, levels = rev(unique(word_count)))) %>%
group_by(year) %>%
top_n(10) %>%
ungroup() %>%
arrange(year, tf_idf) %>%
mutate(order = row_number())
ggplot(pd, aes(order, tf_idf, fill = tf_idf)) +
geom_bar(show.legend = FALSE, stat = "identity") +
labs(x = NULL, y = "TF-IDF value") +
facet_wrap(~year, ncol = 2, scales = "free") +
scale_x_continuous(breaks = pd$order, labels = pd$word, expand = c(0,0)) +
scale_y_continuous(expand = c(0,0)) +
coord_flip() +
theme_minimal() +
scale_fill_viridis_c(direction=-1)
statements_words_year <- statements_words %>%
group_by(year, word_count) %>%
summarize(n = sum(n)) %>%
bind_tf_idf(word_count, year, n) %>%
arrange(desc(tf_idf)) %>%
ungroup()
pd <- statements_words_year %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word_count, levels = rev(unique(word_count)))) %>%
group_by(year) %>%
top_n(10) %>%
ungroup() %>%
arrange(year, tf_idf) %>%
mutate(order = row_number())
ggplot(pd, aes(order, tf_idf, fill = tf_idf)) +
geom_bar(show.legend = FALSE, stat = "identity") +
labs(x = NULL, y = "TF-IDF value") +
facet_wrap(~year, ncol = 2, scales = "free") +
scale_x_continuous(breaks = pd$order, labels = pd$word, expand = c(0,0)) +
scale_y_continuous(expand = c(0,0)) +
coord_flip() +
theme_minimal() +
scale_fill_viridis_c(direction=-1)
tidy_statement <- statements_clean %>%
mutate(year = year(Date)) %>%
group_by(year) %>%
ungroup() %>%
unnest_tokens(word, cleaned_text)
tidy_statement <- tidy_statement %>%
select(year, ID, word)
View(tidy_statement)
loadDictionaryLM()
tdm_tm <- DocumentTermMatrix(corpus_clean)
ap_lda <- LDA(tdm_tm, k = 4, control = list(seed = 1234))
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
top_n(8, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE, fill = viridis(36)) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(y="Value of beta parameter", x="") +
scale_x_reordered() +
theme_minimal()
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
top_n(8, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE, fill = viridis(32)) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(y="Value of beta parameter", x="") +
scale_x_reordered() +
theme_minimal()
by_chapter <- statements_clean %>%
group_by(ID)
by_chapter_word <- by_chapter %>%
unnest_tokens(word, cleaned_text)
word_counts <- by_chapter_word %>%
anti_join(stop_words) %>%
count(ID, word, sort = TRUE)
chapters_dtm <- word_counts %>%
cast_dtm(ID, word, n)
chapters_lda <- LDA(chapters_dtm, k = 4, control = list(seed = 1234))
chapters_lda_td <- tidy(chapters_lda)
chapters_lda_gamma <- tidy(chapters_lda, matrix = "gamma")
topic_1 = chapters_lda_gamma %>% filter(., topic=='1')
topic_2 = chapters_lda_gamma %>% filter(., topic=='2')
topic_3 = chapters_lda_gamma %>% filter(., topic=='3')
topic_4 = chapters_lda_gamma %>% filter(., topic=='4')
topic_1 = topic_1 %>% rename('topic1' = 'gamma')
topic_2 = topic_2 %>% rename('topic2' = 'gamma')
topic_3 = topic_3 %>% rename('topic3' = 'gamma')
topic_4 = topic_4 %>% rename('topic4' = 'gamma')
statements_topics = merge(statements_clean, topic_1, by.x = "ID", by.y = "document")
statements_topics = merge(statements_topics, topic_2, by.x = "ID", by.y = "document")
statements_topics = merge(statements_topics, topic_3, by.x = "ID", by.y = "document")
statements_topics = merge(statements_topics, topic_4, by.x = "ID", by.y = "document")
drops <- c("topic.x", "topic.y")
statements_topics = statements_topics[ , !(names(statements_topics) %in% drops)]
colors_viridis <- viridis(4)
topic_density <- ggplot(statements_topics, aes(x=Date)) +
geom_line(aes(y=topic1), color=colors_viridis[1]) +
geom_line(aes(y=topic2), color=colors_viridis[2]) +
geom_line(aes(y=topic3), color=colors_viridis[3]) +
geom_line(aes(y=topic4), color=colors_viridis[4]) +
scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
theme_minimal()
ggplotly(topic_density)
